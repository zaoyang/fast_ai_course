{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/ml_py3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%cd ~/nbs\n",
    "import sys, os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "sys.path.append(os.path.join(os.getcwd(), \"part2_orig\"))\n",
    "import importlib\n",
    "import utils2; importlib.reload(utils2)\n",
    "from utils2 import *\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "from keras import metrics\n",
    "import os\n",
    "from vgg16_avg import VGG16_Avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tell Tensorflow to use no more GPU RAM than necessary\n",
    "#limit_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs\n",
      "/home/ubuntu/nbs/data/imagenet/sample/\n",
      "/home/ubuntu/nbs/data/\n"
     ]
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "print(pwd)\n",
    "\n",
    "path = pwd + '/data/imagenet/sample/'\n",
    "dpath = pwd + '/data/'\n",
    "resultsPath = pwd + '/data/output/'\n",
    "print(path)\n",
    "print(dpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_preprocess(img_ar):\n",
    "    '''\n",
    "    Input: image as numpy array\n",
    "    Output: preprocessed image as numpy array\n",
    "    '''\n",
    "    resnet_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "    exp_img_ar = np.expand_dims(np.array(img_ar), 0)\n",
    "    proc_img_ar = (exp_img_ar - resnet_mean)[:,:,:,::-1]\n",
    "    return proc_img_ar\n",
    "#end\n",
    "\n",
    "def image_postprocess(img_ar, shp):\n",
    "    '''Input: preprocessed image as numpy array\n",
    "       Output: postprocessed image as numpy array\n",
    "    '''\n",
    "    resnet_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "    postpr_img_ar = np.clip(img_ar.reshape(shp)[:,:,:,::-1] + resnet_mean, 0, 255)\n",
    "    return postpr_img_ar\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resize_to = (500, 350)\n",
    "\n",
    "vangogh_style = Image.open(dpath + 'van_gogh_03.jpg').resize(resize_to)\n",
    "leaningtower_content = Image.open(dpath + 'mangrove_bay.JPG').resize(resize_to)\n",
    "\n",
    "vangogh_style_ar = image_preprocess(vangogh_style)\n",
    "leaningtower_content_ar = image_preprocess(leaningtower_content)\n",
    "\n",
    "shp = vangogh_style_ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_avgpooling(vgg_model):\n",
    "    vgg_avg_model = Sequential()\n",
    "    for i, layer in enumerate(vgg_model.layers):\n",
    "        name = layer.name\n",
    "        if type(layer)!=MaxPooling2D:\n",
    "            vgg_avg_model.add(layer)\n",
    "        else:\n",
    "            vgg_avg_model.add(AveragePooling2D((2, 2), strides=(2, 2), name=name[0:6] + '_avgpool'))\n",
    "        #end\n",
    "    #end\n",
    "    return vgg_avg_model\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_loss(computed, target, weight_ls=None):\n",
    "    '''\n",
    "    Input: computed and target tensors (or lists of tensors for more than one content layer)\n",
    "    Output: content loss calculated as MSE and scaled by the tensor(s) dimension\n",
    "    '''\n",
    "    if isinstance(computed, list):\n",
    "        if not weight_ls:\n",
    "            weight_ls = [1.0 for layer in computed]\n",
    "        #end\n",
    "        c_loss = sum([K.sum(metrics.mse(comp[0], targ[0]) * w \\\n",
    "                      for comp, targ, w in zip(computed, target, weight_ls))])\n",
    "        _, height, width, channels = map(lambda i: i, K.int_shape(computed[0]))\n",
    "    else:\n",
    "        c_loss = K.sum(metrics.mse(computed, target))\n",
    "        _, height, width, channels = K.int_shape(computed)\n",
    "    #end\n",
    "    c_loss = c_loss / (height * width * channels)\n",
    "    return c_loss\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(tens):\n",
    "    features = K.batch_flatten(K.permute_dimensions(tens, (2, 0, 1))) # rows = channels, columns = flattened x, y\n",
    "    gram = K.dot(features, K.transpose(features)) / tens.get_shape().num_elements() #correlate each pair of channels\n",
    "    return gram\n",
    "#end\n",
    "\n",
    "def style_loss(computed, target, weight_ls=None):\n",
    "    '''\n",
    "    Input: computed and target tensors (or lists of tensors for more than one style layer)\n",
    "    Output: content loss calculated as MSE of the Gram matrices and scaled by the tensor(s) dimension\n",
    "    '''\n",
    "    if isinstance(computed, list):\n",
    "        if not weight_ls:\n",
    "            weight_ls = [1.0 for layer in computed]\n",
    "        #end\n",
    "        s_loss = sum([K.sum(metrics.mse(gram_matrix(comp[0]), gram_matrix(targ[0]))) * w \\\n",
    "                      for comp, targ, w in zip(computed, target, weight_ls)])\n",
    "        _, height, width, channels = map(lambda i: i, K.int_shape(computed[0]))\n",
    "    else:\n",
    "        s_loss = K.sum(metrics.mse(gram_matrix(computed), gram_matrix(target)))\n",
    "        _, height, width, channels = K.int_shape(computed)\n",
    "    #end\n",
    "    s_loss = s_loss / (height * width * channels)\n",
    "    return s_loss\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_loss(style_layer_ls, style_targ_ls, style_wgt_ls, content_layer, content_targ, style2content_ratio):\n",
    "    s_loss = style_loss(style_layer_ls, style_targ_ls, style_wgt_ls)\n",
    "    c_loss = content_loss(content_layer, content_targ)\n",
    "    loss = s_loss + c_loss / style2content_ratio\n",
    "    return loss\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_content_targets(style_transfer_model, content_ar):\n",
    "    '''\n",
    "    Input: style transfer model and numpy array of the content image\n",
    "    Output: the output of the model at the content layer and its respective target\n",
    "    '''\n",
    "    layer_output_dc = {l.name: l.get_output_at(0) for l in style_transfer_model.layers}\n",
    "    content_layer = layer_output_dc['block4_conv2'] #change it to another layer of choice if necessary\n",
    "    content_model = Model(style_transfer_model.input, content_layer)\n",
    "    content_targ = K.variable(content_model.predict(content_ar))\n",
    "    return content_layer, content_targ\n",
    "#end \n",
    "\n",
    "def get_style_targets(style_transfer_model, style_ar):\n",
    "    '''Input: style transfer model and numpy array of the style image\n",
    "       Output: the output of the model at the style layer(s) and its respective target\n",
    "    '''\n",
    "    layer_output_dc = {l.name: l.get_output_at(0) for l in style_transfer_model.layers}\n",
    "    style_layer_ls = [layer_output_dc['block{}_conv2'.format(o)] for o in range(1,6)] #change it different layers if necessary\n",
    "    style_model = Model(style_transfer_model.input, style_layer_ls)\n",
    "    style_targ_ls = [K.variable(o) for o in style_model.predict(style_ar)]\n",
    "    return style_layer_ls, style_targ_ls\n",
    "#end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "    '''\n",
    "    Initialization: function and shape of the image array\n",
    "    Returns the loss and the gradients as computed with\n",
    "       respect to the image that is fed to the CNN\n",
    "    '''\n",
    "    def __init__(self, f, shp):\n",
    "        self.f = f\n",
    "        self.shp = shp\n",
    "        return\n",
    "    #end        \n",
    "    def loss(self, x):\n",
    "        loss_, self.grad_values = self.f([x.reshape(self.shp)])\n",
    "        return loss_.astype(np.float64)\n",
    "    #end\n",
    "    def grads(self, x): \n",
    "        return self.grad_values.flatten().astype(np.float64)\n",
    "    #end    \n",
    "#end\n",
    "\n",
    "def apply_transfer(eval_obj, n_iter, img, shp, pref='', save=True, verbose=True):\n",
    "    '''\n",
    "    Input: evaluator, number of iterations, input image and shape\n",
    "    Output: final image, list of losses and info dictionary of optimization procedure\n",
    "    '''\n",
    "    info_dc = dict()\n",
    "    loss_ls = list()\n",
    "    for it in range(n_iter):\n",
    "        img, min_val, iter_dc = fmin_l_bfgs_b(eval_obj.loss, img.flatten(),\n",
    "                                              fprime=eval_obj.grads, maxfun=20)\n",
    "        img = np.clip(img, -127, 127)\n",
    "        info_dc['iteration_'+str(it+1)] = iter_dc\n",
    "        loss_ls = loss_ls + [min_val]\n",
    "        if verbose:\n",
    "            print('Current loss value:', min_val)\n",
    "        #end\n",
    "        if save:\n",
    "            imsave(resultsPath + pref + 'res_at_iteration_' + str(it+1) + '.png', image_postprocess(img.copy(), shp)[0])\n",
    "        #end\n",
    "    #end\n",
    "    return img, loss_ls, info_dc\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss value: 7.14779520035\n",
      "Current loss value: 6.42948675156\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159507751\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159507751\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159507751\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159507751\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159507751\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159603119\n",
      "Current loss value: 6.44159507751\n",
      "Current loss value: 6.44159603119\n"
     ]
    }
   ],
   "source": [
    "iterations = 50\n",
    "\n",
    "st_transf_model = vgg_avgpooling(VGG16(include_top=False, input_shape=shp[1:]))\n",
    "\n",
    "content_layer, content_targ = get_content_targets(st_transf_model, leaningtower_content_ar)\n",
    "style_layer_ls, style_targ_ls = get_style_targets(st_transf_model, vangogh_style_ar)\n",
    "\n",
    "style_wgt_ls = [0.05,0.2,0.2,0.25,0.3]\n",
    "style2content_ratio = 15.0\n",
    "loss = total_loss(style_layer_ls, style_targ_ls, style_wgt_ls, content_layer, content_targ, style2content_ratio)\n",
    "grads = K.gradients(loss, st_transf_model.input)\n",
    "transfer_fn = K.function([st_transf_model.input], [loss] + grads)\n",
    "evaluator = Evaluator(transfer_fn, shp)\n",
    "\n",
    "virgin_img = np.random.uniform(-2.5, 2.5, shp)/100\n",
    "\n",
    "image, t_loss, info_dc = apply_transfer(evaluator, iterations, virgin_img, shp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml_py3]",
   "language": "python",
   "name": "conda-env-ml_py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
