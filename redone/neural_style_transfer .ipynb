{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%cd ~/nbs\n",
    "import sys, os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "sys.path.append(os.path.join(os.getcwd(), \"part2_orig\"))\n",
    "import importlib\n",
    "import utils2; importlib.reload(utils2)\n",
    "from utils2 import *\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "from keras import metrics\n",
    "import os\n",
    "from vgg16_avg import VGG16_Avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tell Tensorflow to use no more GPU RAM than necessary\n",
    "# limit_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs\n",
      "/home/ubuntu/nbs/data/imagenet/sample/\n",
      "/home/ubuntu/nbs/data/\n"
     ]
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "print(pwd)\n",
    "\n",
    "path = pwd + '/data/imagenet/sample/'\n",
    "dpath = pwd + '/data/'\n",
    "resultsPath = pwd + '/data/output/'\n",
    "print(path)\n",
    "print(dpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_preprocess(img_ar):\n",
    "    '''\n",
    "    Input: image as numpy array\n",
    "    Output: preprocessed image as numpy array\n",
    "    '''\n",
    "    resnet_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "    exp_img_ar = np.expand_dims(np.array(img_ar), 0)\n",
    "    proc_img_ar = (exp_img_ar - resnet_mean)[:,:,:,::-1]\n",
    "    return proc_img_ar\n",
    "#end\n",
    "\n",
    "def image_postprocess(img_ar, shp):\n",
    "    '''Input: preprocessed image as numpy array\n",
    "       Output: postprocessed image as numpy array\n",
    "    '''\n",
    "    resnet_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "    postpr_img_ar = np.clip(img_ar.reshape(shp)[:,:,:,::-1] + resnet_mean, 0, 255)\n",
    "    return postpr_img_ar\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resize_to = (500, 350)\n",
    "\n",
    "vangogh_style = Image.open(dpath + 'munch.jpeg').resize(resize_to)\n",
    "leaningtower_content = Image.open(dpath + 'trump.jpg').resize(resize_to)\n",
    "\n",
    "vangogh_style_ar = image_preprocess(vangogh_style)\n",
    "leaningtower_content_ar = image_preprocess(leaningtower_content)\n",
    "\n",
    "shp = vangogh_style_ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_avgpooling(vgg_model):\n",
    "    vgg_avg_model = Sequential()\n",
    "    for i, layer in enumerate(vgg_model.layers):\n",
    "        name = layer.name\n",
    "        if type(layer)!=MaxPooling2D:\n",
    "            vgg_avg_model.add(layer)\n",
    "        else:\n",
    "            vgg_avg_model.add(AveragePooling2D((2, 2), strides=(2, 2), name=name[0:6] + '_avgpool'))\n",
    "        #end\n",
    "    #end\n",
    "    return vgg_avg_model\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_loss(computed, target, weight_ls=None):\n",
    "    '''\n",
    "    Input: computed and target tensors (or lists of tensors for more than one content layer)\n",
    "    Output: content loss calculated as MSE and scaled by the tensor(s) dimension\n",
    "    '''\n",
    "    if isinstance(computed, list):\n",
    "        if not weight_ls:\n",
    "            weight_ls = [1.0 for layer in computed]\n",
    "        #end\n",
    "        c_loss = sum([K.sum(metrics.mse(comp[0], targ[0]) * w \\\n",
    "                      for comp, targ, w in zip(computed, target, weight_ls))])\n",
    "        _, height, width, channels = map(lambda i: i, K.int_shape(computed[0]))\n",
    "    else:\n",
    "        c_loss = K.sum(metrics.mse(computed, target))\n",
    "        _, height, width, channels = K.int_shape(computed)\n",
    "    #end\n",
    "    c_loss = c_loss #/ (height * width * channels)\n",
    "    return c_loss\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(tens):\n",
    "    features = K.batch_flatten(K.permute_dimensions(tens, (2, 0, 1))) # rows = channels, columns = flattened x, y\n",
    "    gram = K.dot(features, K.transpose(features)) / tens.get_shape().num_elements() #correlate each pair of channels\n",
    "    return gram\n",
    "#end\n",
    "\n",
    "def style_loss(computed, target, weight_ls=None):\n",
    "    '''\n",
    "    Input: computed and target tensors (or lists of tensors for more than one style layer)\n",
    "    Output: content loss calculated as MSE of the Gram matrices and scaled by the tensor(s) dimension\n",
    "    '''\n",
    "    if isinstance(computed, list):\n",
    "        if not weight_ls:\n",
    "            weight_ls = [1.0 for layer in computed]\n",
    "        #end\n",
    "        s_loss = sum([K.sum(metrics.mse(gram_matrix(comp[0]), gram_matrix(targ[0]))) * w \\\n",
    "                      for comp, targ, w in zip(computed, target, weight_ls)])\n",
    "        _, height, width, channels = map(lambda i: i, K.int_shape(computed[0]))\n",
    "    else:\n",
    "        s_loss = K.sum(metrics.mse(gram_matrix(computed), gram_matrix(target)))\n",
    "        _, height, width, channels = K.int_shape(computed)\n",
    "    #end\n",
    "    s_loss = s_loss #/ (height * width * channels)\n",
    "    return s_loss\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_loss(style_layer_ls, style_targ_ls, style_wgt_ls, content_layer, content_targ, style2content_ratio):\n",
    "    s_loss = style_loss(style_layer_ls, style_targ_ls, style_wgt_ls)\n",
    "    c_loss = content_loss(content_layer, content_targ)\n",
    "    loss = s_loss + c_loss / style2content_ratio\n",
    "    return loss\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_content_targets(style_transfer_model, content_ar):\n",
    "    '''\n",
    "    Input: style transfer model and numpy array of the content image\n",
    "    Output: the output of the model at the content layer and its respective target\n",
    "    '''\n",
    "    layer_output_dc = {l.name: l.get_output_at(0) for l in style_transfer_model.layers}\n",
    "    content_layer = layer_output_dc['block4_conv2'] #change it to another layer of choice if necessary\n",
    "    content_model = Model(style_transfer_model.input, content_layer)\n",
    "    content_targ = K.variable(content_model.predict(content_ar))\n",
    "    return content_layer, content_targ\n",
    "#end \n",
    "\n",
    "def get_style_targets(style_transfer_model, style_ar):\n",
    "    '''Input: style transfer model and numpy array of the style image\n",
    "       Output: the output of the model at the style layer(s) and its respective target\n",
    "    '''\n",
    "    layer_output_dc = {l.name: l.get_output_at(0) for l in style_transfer_model.layers}\n",
    "    style_layer_ls = [layer_output_dc['block{}_conv2'.format(o)] for o in range(1,6)] #change it different layers if necessary\n",
    "    style_model = Model(style_transfer_model.input, style_layer_ls)\n",
    "    style_targ_ls = [K.variable(o) for o in style_model.predict(style_ar)]\n",
    "    return style_layer_ls, style_targ_ls\n",
    "#end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "    '''\n",
    "    Initialization: function and shape of the image array\n",
    "    Returns the loss and the gradients as computed with\n",
    "       respect to the image that is fed to the CNN\n",
    "    '''\n",
    "    def __init__(self, f, shp):\n",
    "        self.f = f\n",
    "        self.shp = shp\n",
    "        return\n",
    "    #end        \n",
    "    def loss(self, x):\n",
    "        loss_, self.grad_values = self.f([x.reshape(self.shp)])\n",
    "        return loss_.astype(np.float64)\n",
    "    #end\n",
    "    def grads(self, x): \n",
    "        return self.grad_values.flatten().astype(np.float64)\n",
    "    #end    \n",
    "#end\n",
    "\n",
    "def apply_transfer(eval_obj, n_iter, img, shp, pref='', save=True, verbose=True):\n",
    "    '''\n",
    "    Input: evaluator, number of iterations, input image and shape\n",
    "    Output: final image, list of losses and info dictionary of optimization procedure\n",
    "    '''\n",
    "    info_dc = dict()\n",
    "    loss_ls = list()\n",
    "    for it in range(n_iter):\n",
    "        img, min_val, iter_dc = fmin_l_bfgs_b(eval_obj.loss, img.flatten(),\n",
    "                                              fprime=eval_obj.grads, maxfun=20)\n",
    "        img = np.clip(img, -127, 127)\n",
    "        info_dc['iteration_'+str(it+1)] = iter_dc\n",
    "        loss_ls = loss_ls + [min_val]\n",
    "        if verbose:\n",
    "            print('Current loss value:', min_val)\n",
    "        #end\n",
    "        if save:\n",
    "            imsave(resultsPath + pref + 'res_at_iteration_' + str(it+1) + '.png', image_postprocess(img.copy(), shp)[0])\n",
    "        #end\n",
    "    #end\n",
    "    return img, loss_ls, info_dc\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss value: 6514186.0\n",
      "Current loss value: 3456853.25\n",
      "Current loss value: 2664727.5\n",
      "Current loss value: 2318071.5\n",
      "Current loss value: 2131818.0\n",
      "Current loss value: 2010079.0\n",
      "Current loss value: 1925805.75\n",
      "Current loss value: 1859138.25\n",
      "Current loss value: 1804408.25\n",
      "Current loss value: 1760254.75\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "\n",
    "st_transf_model = vgg_avgpooling(VGG16(include_top=False, input_shape=shp[1:]))\n",
    "\n",
    "content_layer, content_targ = get_content_targets(st_transf_model, leaningtower_content_ar)\n",
    "style_layer_ls, style_targ_ls = get_style_targets(st_transf_model, vangogh_style_ar)\n",
    "\n",
    "style_wgt_ls = [0.05,0.2,0.2,0.25,0.3]\n",
    "style2content_ratio = 15.0\n",
    "loss = total_loss(style_layer_ls, style_targ_ls, style_wgt_ls, content_layer, content_targ, style2content_ratio)\n",
    "grads = K.gradients(loss, st_transf_model.input)\n",
    "transfer_fn = K.function([st_transf_model.input], [loss] + grads)\n",
    "evaluator = Evaluator(transfer_fn, shp)\n",
    "\n",
    "virgin_img = np.random.uniform(-2.5, 2.5, shp)/100\n",
    "\n",
    "image, t_loss, info_dc = apply_transfer(evaluator, iterations, virgin_img, shp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
