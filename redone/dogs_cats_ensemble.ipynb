{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs\n"
     ]
    }
   ],
   "source": [
    "import os, sys \n",
    "%cd ~/nbs/\n",
    "sys.path.append(os.path.join(os.getcwd(), \"orig\" ))\n",
    "\n",
    "%matplotlib inline \n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create batches of validation \n",
    "2. Create batches of training \n",
    "3. Use average of different models and fit it collectively \n",
    "4. Fit the batches\n",
    "5. Run the training on the train and validation data. \n",
    "6. Predict on the test \n",
    "7. Run prediction on validation set \n",
    "8. See what of the predicted on the validation set is correct \n",
    "9. Submit tests to backend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'data/dogscats/'\n",
    "model_path = path + 'model/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_batches = get_batches(path + 'train', shuffle=False, batch_size=batch_size)\n",
    "validation_batches = get_batches(path + 'valid', shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "??get_classes\n",
    "(val_classes, training_classes, val_labels, training_labels, \n",
    "val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an ensemble of different models\n",
    "\n",
    "1. Train model with last layer all other layer traininable is false\n",
    "2. Add model with everything but the last layer. This creates an ensemble. \n",
    "3. Fine tune just the dense layers of this model. \n",
    "4. Add data augmentation, fine tuning dense layers without computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Vgg16().model\n",
    "# print(model.summary())\n",
    "conv_layers, fc_layers = split_at(model, Convolution2D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "conv_model = Sequential(conv_layers)\n",
    "# print(conv_model.summary())\n",
    "\n",
    "print(validation_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_predictions = conv_model.predict_generator(validation_batches, validation_batches.nb_sample)\n",
    "training_predictions = conv_model.predict_generator(training_batches, training_batches.nb_sample)\n",
    "save_array(model_path + 'valconvlayer_predictions.bc', val_predictions)\n",
    "save_array(model_path + 'trainconvlayer_predictions.bc', training_predictions)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation = get_data(path+'valid')\n",
    "training = get_data(path+'train')\n",
    "save_array(model_path + 'valid_data.bc', validation)\n",
    "save_array(model_path + 'training_data.bc', training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.pop()\n",
    "model.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the prediction without the last two layers\n",
    "# which is dropout and dense \n",
    "ll_val_predictions = model.predict_generator(validation_batches, validation_batches.nb_sample)\n",
    "ll_trn_predictions = model.predict_generator(training_batches, training_batches.nb_sample)\n",
    "\n",
    "save_array(model_path + 'model_val.bc', model_val_predictions)\n",
    "save_array(model_path + 'model_trn.bc', model_trn_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = get_data(path+'test')\n",
    "save_array(model_path + 'test_data.bc', test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = load_array(model_path + 'test_data.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the last last layer. The original model \n",
    "# had dropout, and dense but not BatchNormalization\n",
    "# This is simply adding BatchNormalization in \n",
    "def get_last_layer():\n",
    "    lastLayer = [BatchNormalization(input_shape=(4096,)), \n",
    "                Dropout(0.5),\n",
    "                Dense(2,activation='softmax')]\n",
    "    return lastLayer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_last_layer():\n",
    "    lastLayers = get_last_layer()\n",
    "    ll_model = Sequential(lastLayers)\n",
    "    ll_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    ll_model.optimizer.lr = 1e-5\n",
    "    ll_model.fit(ll_trn_predictions, training_labels, validation_data=(ll_val_predictions, validation_labels), nb_epoch=12)\n",
    "    ll_model.optimizer.lr=1e-7\n",
    "    ll_model.fit(ll_trn_predictions, training_labels, validation_data=(ll_val_predictions, validation_labels), nb_epoch=1)\n",
    "    ll_model.save_weights(model_path+'ll_bn' + i + '.h5')\n",
    "    \n",
    "    vgg = VGG16()\n",
    "    model = vgg.model\n",
    "    model.pop(); model.pop(); model.pop()\n",
    "    for layer in model.layers(): \n",
    "        layer.trainable = False \n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    ll_layers = get_last_layer()\n",
    "    for layer in ll_layers: \n",
    "        model.add(layer)\n",
    "\n",
    "    for layer1, layer2 in zip(ll_model.layers, model.layers[-3:]): \n",
    "        layer2.set_weights(ll.get_weights())\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuacy'])\n",
    "    model.save_weights(model_path + 'bn' + i + '.h5')\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
